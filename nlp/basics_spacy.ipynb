{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://spacy.io\n",
    "\n",
    "### Industrial-Strength Natural Language Processing\n",
    "\n",
    "### Fast and easy integration with deep learning purposes\n",
    "\n",
    "#### Make sure you install the package via `pip3 install spacy` and then download the English language model via `python3 -m spacy download en`\n",
    "\n",
    "Let's see some basic tasks with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "WEBSITE_DES = '''\n",
    "spaCy is the best way to prepare text for deep learning.\n",
    "'''\n",
    "\n",
    "# Load the spacy language model for English\n",
    "# sm at the end stands for small. Some models are missing in the small\n",
    "# version, e.g. word vectors. Use lg to load all of them\n",
    "# en_vectors_web_lg includes over 1 million unique vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Now let's create a document and see how spacy process it for us\n",
    "document = nlp(WEBSITE_DES.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### POS tagging\n",
    "Let's see the tokens and their part of speech (POS) in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy PROPN NNP noun, proper singular\n",
      "is VERB VBZ verb, 3rd person singular present\n",
      "the DET DT determiner\n",
      "best ADJ JJS adjective, superlative\n",
      "way NOUN NN noun, singular or mass\n",
      "to PART TO infinitival to\n",
      "prepare VERB VB verb, base form\n",
      "text NOUN NN noun, singular or mass\n",
      "for ADP IN conjunction, subordinating or preposition\n",
      "deep ADJ JJ adjective\n",
      "learning NOUN NN noun, singular or mass\n",
      ". PUNCT . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for word in document:\n",
    "    # Spacy ships with a POS-tagger without pain\n",
    "    print(word.text, word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the Token type and its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "['__repr__', '__hash__', '__str__', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__len__', '__new__', 'set_extension', 'get_extension', 'has_extension', 'remove_extension', '__unicode__', '__bytes__', '__reduce__', 'check_flag', 'nbor', 'similarity', 'is_ancestor', '_', 'lex_id', 'rank', 'string', 'text', 'text_with_ws', 'prob', 'sentiment', 'lang', 'idx', 'cluster', 'orth', 'lower', 'norm', 'shape', 'prefix', 'suffix', 'lemma', 'pos', 'tag', 'dep', 'has_vector', 'vector', 'vector_norm', 'tensor', 'n_lefts', 'n_rights', 'sent', 'sent_start', 'is_sent_start', 'lefts', 'rights', 'children', 'subtree', 'left_edge', 'right_edge', 'ancestors', 'head', 'conjuncts', 'ent_type', 'ent_type_', 'ent_iob', 'ent_iob_', 'ent_id', 'ent_id_', 'ent_kb_id', 'ent_kb_id_', 'whitespace_', 'orth_', 'lower_', 'norm_', 'shape_', 'prefix_', 'suffix_', 'lang_', 'lemma_', 'pos_', 'tag_', 'dep_', 'is_oov', 'is_stop', 'is_alpha', 'is_ascii', 'is_digit', 'is_lower', 'is_upper', 'is_title', 'is_punct', 'is_space', 'is_bracket', 'is_quote', 'is_left_punct', 'is_right_punct', 'is_currency', 'like_url', 'like_num', 'like_email', 'vocab', 'i', 'doc', '__doc__', '__pyx_vtable__', '__getattribute__', '__setattr__', '__delattr__', '__init__', '__reduce_ex__', '__subclasshook__', '__init_subclass__', '__format__', '__sizeof__', '__dir__', '__class__']\n"
     ]
    }
   ],
   "source": [
    "# Let's see what else each token has\n",
    "print(type(document[-1]))\n",
    "print(document[-1].__dir__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`has_vector` actually refers to the word vector that ships with each\n",
    "real English word when we load the language model.\n",
    "\n",
    "Let's check for dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy nsubj\n",
      "is ROOT\n",
      "n't neg\n",
      "the det\n",
      "best amod\n",
      "way attr\n",
      "to aux\n",
      "prepare relcl\n",
      "text dobj\n",
      "for prep\n",
      "deep amod\n",
      "learning pobj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "doc_copy = nlp(WEBSITE_DES.replace('is', 'isn\\'t').strip())\n",
    "for word in doc_copy:\n",
    "    print(word, word.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy excels at large-scale information extraction tasks.\n",
      "It's written from the ground up in carefully memory-managed Cython.\n",
      "Independent research in 2015 found spaCy to be the fastest in the world.\n"
     ]
    }
   ],
   "source": [
    "WEBSITE_DES = '''spaCy excels at large-scale information extraction tasks. It's written from the ground up in carefully memory-managed Cython. Independent research in 2015 found spaCy to be the fastest in the world.'''\n",
    "doc_copy = nlp(WEBSITE_DES)\n",
    "for sentence in doc_copy.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending sentence terminators\n",
    "\n",
    "Now imagine a case where you want to process a postmodern poem text where some sentences might end with --- instead of .\n",
    "\n",
    "In this case we can extend the default sentence splitter of `Spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hug the life--- Feed the\n",
      "hope--- Never mind the rest---\n",
      "Make your best---\n",
      "The life is yours--- Like other alls.\n",
      "And a sentence end with period.\n",
      "And another.\n"
     ]
    }
   ],
   "source": [
    "# First let's see if that works already\n",
    "poem = nlp('Hug the life--- Feed the hope--- Never mind the rest--- Make your best--- The life is yours--- Like other alls. And a sentence end with period. And another.')\n",
    "for life_hack in poem.sents:\n",
    "    print(life_hack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very clean ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f3a02673b70>),\n",
      " ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f3a020411c8>),\n",
      " ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f3a02041228>)]\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f3a02673b70>),\n",
      " ('three_dots_sentence', <function three_dots_sentence at 0x7f3a01debd90>),\n",
      " ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f3a020411c8>),\n",
      " ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f3a02041228>)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def three_dots_sentence(document):\n",
    "    for token in document[:-1]:\n",
    "        if token.text.endswith('---'):\n",
    "            document[token.i + 1].is_sent_start = True\n",
    "    return document\n",
    "\n",
    "pprint(nlp.pipeline)\n",
    "nlp.add_pipe(three_dots_sentence, before='parser')\n",
    "pprint(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hug the life---\n",
      "Feed the hope---\n",
      "Never mind the rest---\n",
      "Make your best---\n",
      "The life is yours---\n",
      "Like other alls.\n",
      "And a sentence end with period.\n",
      "And another.\n"
     ]
    }
   ],
   "source": [
    "# Let's see the result\n",
    "\n",
    "# New poem object with the updated nlp\n",
    "poem = nlp(poem.text)\n",
    "for life_hack in poem.sents:\n",
    "    print(life_hack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simple, but does the job! However, this is not the best way to achieve our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Drawbacks to Spacy's tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "a\n",
      "non\n",
      "-\n",
      "vegetarian\n",
      "student\n",
      "and\n",
      "zamanias@mcmaster.ca\n",
      "is\n",
      "my\n",
      "email\n",
      "address\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp('I am a non-vegetarian student and zamanias@mcmaster.ca is my email address.')\n",
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Not intelligent enough to recognize non-vegetarian as one word, but\n",
    "intelligent enough to recognize email address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's dive into entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York - GPE - Countries, cities, states\n",
      "the United States - GPE - Countries, cities, states\n",
      "$3 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc_copy = nlp('New York is a city in the United States. I have a $3 million apartment there.')\n",
    "for entity in doc_copy.ents:\n",
    "    print(' - '.join([entity.text, entity.label_,\n",
    "                     str(spacy.explain(entity.label_))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You may also define new entities, e.g. the name of your company, by\n",
    "using the `Span` method from `spacy.tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How about names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein\n",
      "Marilyn Monroe\n",
      "Royal Albert Hall\n"
     ]
    }
   ],
   "source": [
    "doc_copy = nlp('Albert Einstein and Marilyn Monroe married in Royal Albert Hall last night')\n",
    "for noun in doc_copy.noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pretty good results, hmm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"df922063a272402ab6e150b6b0875130-0\" class=\"displacy\" width=\"650\" height=\"337.0\" direction=\"ltr\" style=\"max-width: none; height: 337.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Python</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">good</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">programming</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">language.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df922063a272402ab6e150b6b0875130-0-0\" stroke-width=\"2px\" d=\"M62,202.0 62,185.33333333333334 141.0,185.33333333333334 141.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df922063a272402ab6e150b6b0875130-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,204.0 L58,196.0 66,196.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df922063a272402ab6e150b6b0875130-0-1\" stroke-width=\"2px\" d=\"M262,202.0 262,152.0 547.0,152.0 547.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df922063a272402ab6e150b6b0875130-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M262,204.0 L258,196.0 266,196.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df922063a272402ab6e150b6b0875130-0-2\" stroke-width=\"2px\" d=\"M362,202.0 362,168.66666666666666 544.0,168.66666666666666 544.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df922063a272402ab6e150b6b0875130-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,204.0 L358,196.0 366,196.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df922063a272402ab6e150b6b0875130-0-3\" stroke-width=\"2px\" d=\"M462,202.0 462,185.33333333333334 541.0,185.33333333333334 541.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df922063a272402ab6e150b6b0875130-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M462,204.0 L458,196.0 466,196.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df922063a272402ab6e150b6b0875130-0-4\" stroke-width=\"2px\" d=\"M162,202.0 162,135.33333333333331 550.0,135.33333333333331 550.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df922063a272402ab6e150b6b0875130-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550.0,204.0 L554.0,196.0 546.0,196.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "doc_copy = nlp('Python is a good programming language.')\n",
    "displacy.render(doc_copy, style='dep', jupyter=True,\n",
    "                options={'distance': 100, 'compact':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    McMaster\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is a cool university. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Hamilton\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is a cool city near the cooler \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Toronto\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " city in the coolest country, \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Canada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_copy = nlp('McMaster is a cool university. Hamilton is a cool city near the cooler Toronto city in the coolest country, Canada!')\n",
    "displacy.render(doc_copy, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redaction and Sanitization\n",
    "\n",
    "Sometimes it is necessary to redact names and places from a report before releasing it. `Spacy` can help with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[REDACTED] is a cool university . [REDACTED] is a cool city near the cooler [REDACTED] city in the coolest country , [REDACTED] !'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_redacted = []\n",
    "\n",
    "# New and York are two tokens that form one entity. Let's make them one token.\n",
    "for ent in doc_copy.ents:\n",
    "    ent.merge()\n",
    "    \n",
    "for token in doc_copy:\n",
    "    if token.ent_type_ in ['PERSON', 'ORG', 'GPE']:\n",
    "        doc_redacted.append('[REDACTED]')\n",
    "    else:\n",
    "        doc_redacted.append(token.text)\n",
    "\n",
    "' '.join(doc_redacted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Let's dive into Lemmatization and Stemming in Spacy\n",
    "Stemming refers to reducing a word to its root form.\n",
    "For instance, walk, walking, walker, walked, etc. all\n",
    "come from a common root. It's not a good idea to treat them\n",
    "as distinctive words while doing NLP.\n",
    "\n",
    "Lemmatization also refers to pretty much similar task, with\n",
    "a slightly different approach.\n",
    "\n",
    "As for Stemming, we usually chop off the ends of words **in\n",
    "the hope of** achieving our goal (like a heuristic) while in\n",
    "Lemmatization we use vocabulary analysis of the words to\n",
    "return the dictionary form of them.\n",
    "\n",
    "**Enough of theory, let's see them in action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walker\n",
      "walk\n",
      "computer\n",
      "computing\n",
      "have\n",
      "have\n",
      "be\n",
      "-PRON-\n",
      "-PRON-\n"
     ]
    }
   ],
   "source": [
    "doc_copy = nlp('walk walked walker walking computer computing has have are You they')\n",
    "for word in doc_copy:\n",
    "    print(word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A little strange!\n",
    "\n",
    "In what way is walking different from computing when doing lemmatization?\n",
    "\n",
    "Also note that there is no direct way of doing *Stemming* in\n",
    "Spacy, so we need to use another tool for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity\n",
    "\n",
    "Now let's play with the `similarity()` method of Spacy. It basically uses the vector representation of words to compare their similarity with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70593494\n",
      "0.47661957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"dog cat banana\")\n",
    "print(tokens[0].similarity(tokens[1]))\n",
    "print(tokens[0].similarity(tokens[2]))\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_lg\") \n",
    "\n",
    "# tokens = nlp(\"dog cat banana afsdasdkfsd\")\n",
    "# print(tokens[0].similarity(tokens[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the warning tells us, we need to download the large version of the English language model which is ~900 MB. Feel free to try that yourself!\n",
    "\n",
    "Now let's see the similarity between two documents instead of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6350208743120859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# Example from Stackoverflow\n",
    "doc1 = nlp(\"This was very strange argument between american and british person\")\n",
    "doc2 = nlp(\"He was from Japan, but a true English gentleman in my eyes, and another one of the reasons as to why I liked going to school.\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.63 is quite a large similarity between this two non-relevant sentences. What could be the reason?\n",
    "\n",
    "Maybe not using the large language model. What about `stop words` tho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange argument american british person\n",
      "Japan , true English gentleman eyes , reasons liked going school .\n",
      "0.759336364850448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(' '.join([str(t) for t in doc1 if not t.is_stop]))\n",
    "doc2 = nlp(' '.join([str(t) for t in doc2 if not t.is_stop]))\n",
    "\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way that `Spacy` computes the similarity between the two sentences is that it the word embedding of a full sentence is simply the average over all different words. Therefore, some of the word vectors may cancel each other to form a final more similar vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "\n",
    "We just talked about `stop words`. These are the words that are necessary to form our sentences in a correct and structured way, but they usually don't carry much meaning with themselves, especially in the context of Natural Language Processing.\n",
    "\n",
    "Let's see some of the English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have', 'much', 'anywhere', 'together', 'whole', 'thereafter', 'via', 'hereafter', 'between', 'on', 'they', 'our', 'been', 'everyone', 'of', 'must', 'am', 'we', 'were', 'what', 'alone', 'again', 'name', 'give', 'below', 'toward', 'n‘t', 'whatever', 'up', 'except', 'meanwhile', 'n’t', 'which', 'for', 'quite', 'put', 'who', 'everything', 'whither', 'beside', 'once', 'by', 'least', 'become', 'both', 'call', 'hers', 'so', 'mostly', 'somehow', 'twelve', \"'ll\", 'to', 'formerly', 'within', 'us', 'thence', 'a', 'three', 'upon', 'more', 'ourselves', 'rather', \"'s\", 'further', 'serious', 'none', 'all', 'hereupon', 'made', 'everywhere', 'your', 'last', 'became', 'should', 'ten', '’m', 'into', '’ve', 'forty', 'since', 'sometime', 'whence', 'every', 'its', 'many', 'the', 'most', 'do', 're', 'same', \"n't\", 'is', 'thru', 'she', 'them', 'no', 'then', 'two', 'themselves', 'being', '’d', 'whereafter', 'those', 'twenty', \"'m\", 'very', 'another', 'one', 'other', 'latterly', 'thereupon', 'say', 'in', 'because', 'with', 'thus', 'next', 'back', 'due', 'moreover', 'whether', 'namely', 'either', 'has', 'such', 'under', 'now', 'regarding', 'this', 'seeming', 'otherwise', 'whoever', 'it', 'nobody', 'anyone', 'besides', 'across', 'might', 'may', 'perhaps', 'several', 'and', 'that', 'becoming', '’s', 'nothing', 'five', 'though', 'few', 'there', 'does', 'can', 'against', 'nor', 'ours', 'elsewhere', 'although', 'behind', 'still', '‘m', 'whereby', 'be', 'wherein', 'their', 'will', 'during', 'sometimes', 'bottom', 'whom', 'himself', 'also', 'herself', 'about', 'before', 'each', 'itself', 'thereby', 'cannot', 'could', 'from', 'seemed', 'over', 'afterwards', 'less', 'someone', 'sixty', 'latter', 'mine', '‘re', 'side', 'hundred', 'through', 'yourself', '’ll', 'he', '’re', 'how', 'eleven', 'amongst', 'i', 'somewhere', 'fifty', 'too', 'above', 'move', 'whereas', '‘d', 'at', 'his', 'four', 'among', 'per', 'these', 'are', 'not', 'out', 'ca', 'show', 'part', 'but', 'top', 'anything', 'well', 'various', 'here', \"'d\", 'take', 'keep', 'amount', 'often', 'where', 'really', 'wherever', 'had', 'becomes', 'others', 'until', 'doing', 'enough', 'towards', 'an', 'yours', '‘ve', 'nevertheless', 'onto', 'whenever', 'seem', 'something', 'herein', \"'ve\", 'off', 'fifteen', 'noone', 'her', 'full', 'my', 'or', 'get', 'throughout', 'anyway', 'beforehand', 'while', 'whose', 'some', 'therein', 'done', 'make', 'me', 'already', 'seems', 'than', 'empty', 'myself', 'hence', 'neither', 'if', 'whereupon', 'former', \"'re\", 'using', '‘ll', 'therefore', 'just', 'third', 'nowhere', 'see', 'eight', 'around', 'anyhow', 'first', 'own', 'else', 'front', 'nine', '‘s', 'did', 'never', 'any', 'please', 'six', 'as', 'indeed', 'unless', 'would', 'you', 'ever', 'only', 'was', 'yet', 'go', 'yourselves', 'after', 'used', 'however', 'hereby', 'him', 'always', 'beyond', 'why', 'almost', 'without', 'along', 'down', 'even', 'when'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending Stop Words\n",
    "\n",
    "A lot of times we want to process informal texts, e.g. tweets, reviews. The above stopwords are mostly useful only for formally written English texts such as newspapers, engineering books etc.\n",
    "\n",
    "We can't see *lol*, *hbu*, *lmao*... in the list above. Therefore, it's a good idea to extend the set of stop words based on the application and the problem we are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False \n",
      "\n",
      "lol True\n",
      "you True\n",
      "'re True\n",
      "very True\n",
      "funny False\n",
      "lmao True\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab['lol'].is_stop, '\\n')\n",
    "\n",
    "nlp.vocab['lol'].is_stop = True\n",
    "nlp.vocab['hbu'].is_stop = True\n",
    "nlp.vocab['lmao'].is_stop = True\n",
    "\n",
    "tokens = nlp('lol you\\'re very funny lmao')\n",
    "for token in tokens:\n",
    "    print(token.text, token.is_stop)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (macai)",
   "language": "python",
   "name": "pycharm-b7ca10a9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
