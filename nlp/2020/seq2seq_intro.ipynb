{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://jalammar.github.io/images/seq2seq_2.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('https://jalammar.github.io/images/seq2seq_2.mp4', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to seq2seq models, Attenction and Transformers\n",
    "\n",
    "This presentation is heavily inspired by [Jay Alammar](http://jalammar.github.io/) and [Christopher Olah blog](http://colah.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* Sequence to sequence models are `Deep Learning` models used in many tasks\n",
    "    * Machine Translation\n",
    "    * Text Summarization\n",
    "    * Text Generation\n",
    "* Takes in a sequence of items, and outputs another sequence of items\n",
    "    * Here we focus on words as input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is how a trained seq2seq model works for the task of machine translation**\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_2.mp4\" alt=\"Seq2seq machine translation\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging the black box\n",
    "\n",
    "The model is composed of an **encoder** and a **decoder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "* Takes each input item (word) one by one\n",
    "* Processes them and captures their information\n",
    "* Outputs a *Context* vector as its result of processing the entire input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "* Takes the *Context* vector as its input entirely\n",
    "* Processes it and decode the information to fit into the desired output (another language for machine translation task)\n",
    "* Outputs items (words) one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine translation task, step by step**\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_4.mp4\" alt=\"Seq2seq machine translation step by step\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Context is a vector of numbers, representing the information captured by the encoder from the input\n",
    "    * It's a matter of choice what size it has\n",
    "* Both encoder and decoder are Recurrent Neural Networks under the hood\n",
    "    * Introduced RNNs and specifically, LSTMs in previous series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is how the context vector look like**\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/context.png\" alt=\"Context Vector\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "We discussed word embedding methods `Word2Vec` and `GloVe` in the previous series of tutorials. To summarize, word embedding is used to convert words and sentences into numbers so that we could feed them to neural networks.\n",
    "\n",
    "Seq2seq models and specificall, encoders are not exception and we should embed the document before we feed them to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is how an embedded vector for that sentence looks like**\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/embedding.png\" alt=\"Embedded Vector\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of RNN\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/RNN_1.mp4\" alt=\"RNNs step by step\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hidden state 0 and input vector 1 (current word) are fed to the RNN\n",
    "2. The result of that would be hidden state 1 and output vector 1\n",
    "\n",
    "The unrolled version of RNNs may help to understand their operation better\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"Unrolled RNN\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "3. Similarly, the hidden state 1 and the input vector 2 (next word) are fed to the RNN\n",
    "4. Hidden state 2 and output vector 2 are the outputs\n",
    "5. This process continues until no further input is left\n",
    "\n",
    "The math behind the scenes is a series of dot products and softmax:\n",
    "\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn_step_forward.png\" alt=\"Behind the scenes RNN\" width=\"80%\"/>\n",
    "\n",
    "* W vectors are the weights of the RNN to be trained and optimized\n",
    "* X vector is the embedded word vector (input feature vector)\n",
    "* a vector is the hidden state\n",
    "* y vector is the output state\n",
    "* t and t-1 shows current time step and previous time step, respectively\n",
    "\n",
    "**Note that there is also a backpropagation process for the sake of training the network and adjusting weights, but we don't discuss them here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to encoder-decoder architecture\n",
    "\n",
    "Now that we know how RNNs work, we can continue with the encoder-decoder network.\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_5.mp4\" alt=\"En-De step by step\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each pulse, the RNN in encoder or decoder is processing its input and generating the output and hidden state for that time step.\n",
    "\n",
    "The hiddent states in the encoder RNNs keep propagating to the next ones, until they reach the last RNN in the encoder. The final hidden state vector, will be the `Context Vecror` that goes through the decoder as its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's unroll the process even more.\n",
    "\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_6.mp4\" alt=\"En-De step by step unrolled\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder also works the same way as encoder, as it has a very similar architecture to encoder. However, it does not accept any input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder-decoder weakness and the concept of Attention\n",
    "\n",
    "The `Context vector` tends to be the bottleneck for this model. In the case of long sentences, the number of words is more and when the time step comes to the later words, the hidden state has already forgotten about the earlier words as it propagates throughout the RNN cells.\n",
    "\n",
    "\n",
    "#### Attention\n",
    "\n",
    "Attention helps with the `context vector` bottleneck problem by providing context for **each word** rather than the whole sentence. This helps the decoder to focus on relevant and important parts of the encoded input data at each step of decoding.\n",
    "\n",
    "So the **encoder** with attention sends more information to the decoder by providing **all** of the hiddent states.\n",
    "\n",
    "The **decoder** with attention takes all of the hidden states and do the followings:\n",
    "1. Process the hidden state for each word and gives it a score\n",
    "2. Amplify the important hidden states for each time step and drown the less informative and less important hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is how encoder-decoder with attention works for the task of machine translation**\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_7.mp4\" alt=\"En-De with attention step by step\" width=\"80%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's see how the hidden states pass along decoder cells and how they are scored**\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/attention_process.mp4\" alt=\"Decoder with attention step by step\" width=\"80%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize what happens in the decoder:\n",
    "1. At each time step the previous decoder hidden state is fed to the decoder RNN cell (the decoder RNN input is always /<END/> as we don't have input in decoder)\n",
    "2. The output of the RNN is calculated as new hidden state\n",
    "3. The encoder hidden states are amplified based on their importance against the cell weights\n",
    "4. The result of step 3 and 2 are concatenated to form the final decoder cell hidden state at that time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To visualize how the encoder hidden states are scored, let's look at this example**\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_9.mp4\" alt=\"Translation encoder hidden states scored\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that hidden states are not weighted based on their order, rather based on their importance which does not necessarily comply with the word order*\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/attention_sentence.png\" alt=\"Encoder hidden state amplification\" width=\"80%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-Short Term Memory Networks - LSTM\n",
    "\n",
    "LSTMs are a variation of RNNs that improve the performance. Specifically, they help better preserving the context of previously seen words in future passes. We introduced them with more details in the previous series of tutorials.\n",
    "\n",
    "We use LSTM here to implement a demo. We won't implement the attention mechanism for the sake of time.\n",
    "\n",
    "\n",
    "The example here is heavily inspired by the content from the [Keras blog](https://blog.keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install numpy\n",
    "# !pip install tensorflow\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = './data/fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 30s 239ms/step - loss: 1.1837 - accuracy: 0.7227 - val_loss: 1.0428 - val_accuracy: 0.7023\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 33s 263ms/step - loss: 0.8500 - accuracy: 0.7702 - val_loss: 0.8342 - val_accuracy: 0.7631\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 33s 263ms/step - loss: 0.6785 - accuracy: 0.8087 - val_loss: 0.7097 - val_accuracy: 0.7943\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.5926 - accuracy: 0.8283 - val_loss: 0.6528 - val_accuracy: 0.8077\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.5418 - accuracy: 0.8419 - val_loss: 0.6145 - val_accuracy: 0.8177\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.5055 - accuracy: 0.8516 - val_loss: 0.5778 - val_accuracy: 0.8306\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.4776 - accuracy: 0.8594 - val_loss: 0.5588 - val_accuracy: 0.8346\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.4541 - accuracy: 0.8657 - val_loss: 0.5386 - val_accuracy: 0.8402\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.4331 - accuracy: 0.8713 - val_loss: 0.5206 - val_accuracy: 0.8456\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.4140 - accuracy: 0.8766 - val_loss: 0.5055 - val_accuracy: 0.8496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc1e4326eb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('./data/s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Restez à la mais.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Restez la moi.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Restez la moi.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Laissez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Laissez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui est alle ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Fais son en aite.\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Attends un chanter !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Restez à l'aire.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Restez à la mainon.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Restez à la mainon.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Restez à l'aire.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Restez à l'aire.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Restez à l'aire.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je vous ai sait.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Je n'ai pas pas de moi.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je vous ai sait.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je vous ai sait.\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je me suis pas mainte.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Comment est chante ?\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Restez aller !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Restez aller !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Restez à l'aire.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Restez à l'aire.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Restez à l'aire.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Restez !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Laissez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Laissez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Regardez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Regardez-le !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis sentie.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis sentie.\n",
      "\n",
      "-\n",
      "Input sentence: I fled.\n",
      "Decoded sentence: Je me suis sentie.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je me suis pas mainte.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je me suis sentie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je me suis sentie.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: Je me suis senti.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Je vous ai sait.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Je me suis pas mainte.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je suis sout de la maine.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis sout de la maine.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis sout de la maine.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Laissez-moi la maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Laissez-moi de maine.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Attends un comment !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Attends un comment !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Attends un comment !\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Reste !\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Reste !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous sommes sentir.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes sempes de coure.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes sempes de coure.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes sempes de coure.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes sempes de coure.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Restez la moin.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Les chez soi paste la maine.\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois sentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois sentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois sentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois sentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sontente !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Arrêtez de le mais.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Restez de moi.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Restez de moi.\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Restez de conter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
