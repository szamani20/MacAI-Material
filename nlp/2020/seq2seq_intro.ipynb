{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to seq2seq models, Attenction and Transformers\n",
    "\n",
    "This presentation is heavily inspired by [Jay Alammar](http://jalammar.github.io/) and [Christopher Olah blog](http://colah.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* Sequence to sequence models are `Deep Learning` models used in many tasks\n",
    "    * Machine Translation\n",
    "    * Text Summarization\n",
    "    * Text Generation\n",
    "* Takes in a sequence of items, and outputs another sequence of items\n",
    "    * Here we focus on words as input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is how a trained seq2seq model works for the task of machine translation**\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_2.mp4\" alt=\"Seq2seq machine translation\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging the black box\n",
    "\n",
    "The model is composed of an **encoder** and a **decoder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "* Takes each input item (word) one by one\n",
    "* Processes them and captures their information\n",
    "* Outputs a *Context* vector as its result of processing the entire input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "* Takes the *Context* vector as its input entirely\n",
    "* Processes it and decode the information to fit into the desired output (another language for machine translation task)\n",
    "* Outputs items (words) one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine translation task, step by step**\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_4.mp4\" alt=\"Seq2seq machine translation step by step\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Context is a vector of numbers, representing the information captured by the encoder from the input\n",
    "    * It's a matter of choice what size it has\n",
    "* Both encoder and decoder are Recurrent Neural Networks under the hood\n",
    "    * Introduced RNNs and specifically, LSTMs in previous series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is how the context vector look like**\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/context.png\" alt=\"Context Vector\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "We discussed word embedding methods `Word2Vec` and `GloVe` in the previous series of tutorials. To summarize, word embedding is used to convert words and sentences into numbers so that we could feed them to neural networks.\n",
    "\n",
    "Seq2seq models and specificall, encoders are not exception and we should embed the document before we feed them to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is how an embedded vector for that sentence looks like**\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/embedding.png\" alt=\"Embedded Vector\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of RNN\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/RNN_1.mp4\" alt=\"RNNs step by step\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hidden state 0 and input vector 1 (current word) are fed to the RNN\n",
    "2. The result of that would be hidden state 1 and output vector 1\n",
    "\n",
    "The unrolled version of RNNs may help to understand their operation better\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"Unrolled RNN\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "3. Similarly, the hidden state 1 and the input vector 2 (next word) are fed to the RNN\n",
    "4. Hidden state 2 and output vector 2 are the outputs\n",
    "5. This process continues until no further input is left\n",
    "\n",
    "The math behind the scenes is a series of dot products and softmax:\n",
    "\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn_step_forward.png\" alt=\"Behind the scenes RNN\" width=\"80%\"/>\n",
    "\n",
    "* W vectors are the weights of the RNN to be trained and optimized\n",
    "* X vector is the embedded word vector (input feature vector)\n",
    "* a vector is the hidden state\n",
    "* y vector is the output state\n",
    "* t and t-1 shows current time step and previous time step, respectively\n",
    "\n",
    "**Note that there is also a backpropagation process for the sake of training the network and adjusting weights, but we don't discuss them here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to encoder-decoder architecture\n",
    "\n",
    "Now that we know how RNNs work, we can continue with the encoder-decoder network.\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_5.mp4\" alt=\"En-De step by step\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each pulse, the RNN in encoder or decoder is processing its input and generating the output and hidden state for that time step.\n",
    "\n",
    "The hiddent states in the encoder RNNs keep propagating to the next ones, until they reach the last RNN in the encoder. The final hidden state vector, will be the `Context Vecror` that goes through the decoder as its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's unroll the process even more.\n",
    "\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_6.mp4\" alt=\"En-De step by step unrolled\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder also works the same way as encoder, as it has a very similar architecture to encoder. However, it does not accept any input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder-decoder weakness and the concept of Attention\n",
    "\n",
    "The `Context vector` tends to be the bottleneck for this model. In the case of long sentences, the number of words is more and when the time step comes to the later words, the hidden state has already forgotten about the earlier words as it propagates throughout the RNN cells.\n",
    "\n",
    "\n",
    "#### Attention\n",
    "\n",
    "Attention helps with the `context vector` bottleneck problem by providing context for **each word** rather than the whole sentence. This helps the decoder to focus on relevant and important parts of the encoded input data at each step of decoding.\n",
    "\n",
    "So the **encoder** with attention sends more information to the decoder by providing **all** of the hiddent states.\n",
    "\n",
    "The **decoder** with attention takes all of the hidden states and do the followings:\n",
    "1. Process the hidden state for each word and gives it a score\n",
    "2. Amplify the important hidden states for each time step and drown the less informative and less important hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is how encoder-decoder with attention works for the task of machine translation**\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_7.mp4\" alt=\"En-De with attention step by step\" width=\"80%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's see how the hidden states pass along decoder cells and how they are scored**\n",
    "\n",
    "<video controls src=\"https://jalammar.github.io/images/attention_process.mp4\" alt=\"Decoder with attention step by step\" width=\"80%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize what happens in the decoder:\n",
    "1. At each time step the previous decoder hidden state is fed to the decoder RNN cell (the decoder RNN input is always /<END/> as we don't have input in decoder)\n",
    "2. The output of the RNN is calculated as new hidden state\n",
    "3. The encoder hidden states are amplified based on their importance against the cell weights\n",
    "4. The result of step 3 and 2 are concatenated to form the final decoder cell hidden state at that time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To visualize how the encoder hidden states are scored, let's look at this example**\n",
    "<video controls src=\"https://jalammar.github.io/images/seq2seq_9.mp4\" alt=\"Translation encoder hidden states scored\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that hidden states are not weighted based on their order, rather based on their importance which does not necessarily comply with the word order*\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/attention_sentence.png\" alt=\"Encoder hidden state amplification\" width=\"80%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-Short Term Memory Networks - LSTM\n",
    "\n",
    "LSTMs are a variation of RNNs that improve the performance. Specifically, they help better preserving the context of previously seen words in future passes. We introduced them with more details in the previous series of tutorials.\n",
    "\n",
    "We use LSTM here to implement a demo. We won't implement the attention mechanism for the sake of time.\n",
    "\n",
    "\n",
    "The example here is heavily inspired by the content from the [Keras blog](https://blog.keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
